{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e98ce235",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, Iterable, List, Optional, Set\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "faa35129",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATHS = {\n",
    "    \"DoS\":   \"../../Dataset/DoS_dataset_clean.csv\",\n",
    "    \"Fuzzy\": \"../../Dataset/Fuzzy_dataset_clean.csv\",\n",
    "    \"Gear\":  \"../../Dataset/gear_dataset_clean.csv\",\n",
    "    \"RPM\":   \"../../Dataset/RPM_dataset_clean.csv\",\n",
    "}\n",
    "\n",
    "WINDOW_SIZE = 200\n",
    "WINDOW_STRIDE = 200\n",
    "SAMPLE_RATIO = 0.1\n",
    "QUESTIONS_PER_WINDOW = 3\n",
    "GLOBAL_SEED = 42\n",
    "\n",
    "BYTE_COLUMNS = [f\"Byte{i}\" for i in range(1, 9)]\n",
    "ATTACK_LABELS = [\"DoS\", \"Fuzzy\", \"Gear\", \"RPM\"]\n",
    "HIGH_DLC_RATIO_THRESHOLD = 0.5\n",
    "WINDOW_DURATION_THRESHOLD = 2.0  # seconds\n",
    "RARE_ID_RATIO_THRESHOLD = 0.3\n",
    "\n",
    "SELECTED_DATASETS = [\"DoS\", \"Fuzzy\", \"Gear\", \"RPM\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf1acae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper function\n",
    "\n",
    "def _normalize_flag_series(series: pd.Series) -> pd.Series:\n",
    "    mapped = series.map({\"R\": 0, \"T\": 1})\n",
    "    numeric = pd.to_numeric(series, errors=\"coerce\")\n",
    "    combined = mapped.fillna(numeric).fillna(0).astype(int)\n",
    "    return combined\n",
    "\n",
    "\n",
    "def load_datasets(paths: Dict[str, str]):\n",
    "    datasets: Dict[str, pd.DataFrame] = {}\n",
    "    profiles: Dict[str, dict] = {}\n",
    "\n",
    "    for name, path in paths.items():\n",
    "        csv_path = Path(path)\n",
    "        if not csv_path.exists():\n",
    "            print(f\"[WARN] Dataset {name} not found at {csv_path}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        df = pd.read_csv(csv_path)\n",
    "        if \"Flag\" in df.columns:\n",
    "            df[\"Flag\"] = _normalize_flag_series(df[\"Flag\"])\n",
    "        else:\n",
    "            df[\"Flag\"] = 0\n",
    "\n",
    "        id_counts = df[\"ID\"].value_counts()\n",
    "        expected_ids = set(int(x) for x in id_counts.head(5).index.tolist())\n",
    "        critical_ids = set(int(x) for x in id_counts.head(3).index.tolist())\n",
    "\n",
    "        datasets[name] = df\n",
    "        profiles[name] = {\n",
    "            \"expected_ids\": expected_ids,\n",
    "            \"critical_ids\": critical_ids,\n",
    "            \"attack_label\": name,  \n",
    "            \"unique_id_threshold\": max(10, len(expected_ids)),\n",
    "        }\n",
    "\n",
    "        print(f\"[INFO] Loaded {name}: {len(df)} rows, \"\n",
    "              f\"{len(expected_ids)} expected IDs, {len(critical_ids)} critical IDs.\")\n",
    "\n",
    "    return datasets, profiles\n",
    "\n",
    "\n",
    "def format_window(df: pd.DataFrame, hide_indices: Optional[Iterable[int]] = None) -> str:\n",
    "    hide_set = set(hide_indices or [])\n",
    "    formatted_rows = []\n",
    "    for idx, row in df.iterrows():\n",
    "        byte_vals = [int(row[col]) for col in BYTE_COLUMNS]\n",
    "        flag_repr = \"Flag=? (hidden)\" if idx in hide_set else f\"Flag={int(row['Flag'])}\"\n",
    "        formatted_rows.append(\n",
    "            f\"Timestamp={row['Timestamp']:.6f} | \"\n",
    "            f\"ID={int(row['ID'])} | DLC={int(row['DLC'])} | \"\n",
    "            f\"bytes={byte_vals} | {flag_repr} |\"\n",
    "        )\n",
    "    return \"\\n\".join(formatted_rows)\n",
    "\n",
    "\n",
    "def _has_constant_payload(df: pd.DataFrame) -> bool:\n",
    "    for _, group in df.groupby(\"ID\"):\n",
    "        if len(group) < 2:\n",
    "            continue\n",
    "        if all(group[col].nunique(dropna=False) == 1 for col in BYTE_COLUMNS):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# Create T/F questions\n",
    "def generate_tf_questions(df: pd.DataFrame, profile: dict,\n",
    "                          rng: np.random.Generator) -> List[dict]:\n",
    "    qas: List[dict] = []\n",
    "    total_frames = len(df)\n",
    "    if total_frames == 0:\n",
    "        return qas\n",
    "\n",
    "    id_counts = df[\"ID\"].value_counts()\n",
    "    unique_ids = set(df[\"ID\"].tolist())\n",
    "    byte_matrix = df[BYTE_COLUMNS]\n",
    "    expected_ids: Set[int] = profile.get(\"expected_ids\", set())\n",
    "    critical_ids: Set[int] = profile.get(\"critical_ids\", set())\n",
    "    unique_id_threshold = profile.get(\"unique_id_threshold\", 20)\n",
    "\n",
    "    def make_entry(question: str, condition, context_type: str = \"full\"):\n",
    "        answer = condition if isinstance(condition, str) else (\"True\" if condition else \"False\")\n",
    "        return {\"question\": question, \"answer\": answer, \"context_type\": context_type}\n",
    "\n",
    "    FLOOD_THRESHOLD = 3\n",
    "    FLOOD_SIGNATURE_SHARE = 0.6\n",
    "\n",
    "    qas.append(make_entry(\"All CAN frames in this window have Flag equal to 0.\",\n",
    "                          bool((df[\"Flag\"] == 0).all())))\n",
    "    qas.append(make_entry(\"At least one CAN frame in this window carries a non-zero Flag.\",\n",
    "                          bool((df[\"Flag\"] != 0).any())))\n",
    "    max_id_share = (id_counts.max() / total_frames) if total_frames > 0 else 0\n",
    "    qas.append(make_entry(\"A single CAN ID accounts for more than half of the frames in this window.\",\n",
    "                          bool(max_id_share > 0.5)))\n",
    "    qas.append(make_entry(f\"Some CAN ID appears more than {FLOOD_THRESHOLD} times in this window.\",\n",
    "                          bool((id_counts > FLOOD_THRESHOLD).any())))\n",
    "\n",
    "    qas.append(make_entry(\"Timestamps are strictly non-decreasing across consecutive frames.\",\n",
    "                          bool(df[\"Timestamp\"].is_monotonic_increasing)))\n",
    "    suppression_threshold = 1e-3\n",
    "    flooding_threshold = 5e-4\n",
    "    if total_frames > 1:\n",
    "        diffs = df[\"Timestamp\"].to_numpy()[1:] - df[\"Timestamp\"].to_numpy()[:-1]\n",
    "        has_large_gap = bool((diffs > suppression_threshold).any())\n",
    "        all_small_gaps = bool((diffs < flooding_threshold).all())\n",
    "    else:\n",
    "        has_large_gap = False\n",
    "        all_small_gaps = False\n",
    "    qas.append(make_entry(\n",
    "        f\"Some inter-frame gap exceeds {suppression_threshold} seconds (possible suppression).\",\n",
    "        has_large_gap,\n",
    "    ))\n",
    "    qas.append(make_entry(\n",
    "        f\"All inter-frame gaps stay below {flooding_threshold} seconds (possible flooding).\",\n",
    "        all_small_gaps,\n",
    "    ))\n",
    "\n",
    "    qas.append(make_entry(\n",
    "        \"Some CAN ID transmits an identical payload across the entire window.\",\n",
    "        _has_constant_payload(df),\n",
    "    ))\n",
    "    qas.append(make_entry(\n",
    "        \"At least one expected control ID is missing from this window.\",\n",
    "        bool(expected_ids) and any(eid not in unique_ids for eid in expected_ids),\n",
    "    ))\n",
    "    qas.append(make_entry(\n",
    "        \"At least one data byte in this window is greater than 200.\",\n",
    "        bool((byte_matrix > 200).any().any()),\n",
    "    ))\n",
    "    unique_id_count = len(unique_ids)\n",
    "    qas.append(make_entry(\n",
    "        f\"The number of distinct CAN IDs exceeds {unique_id_threshold} in this window.\",\n",
    "        unique_id_count > unique_id_threshold,\n",
    "    ))\n",
    "    high_dlc_ratio = (df[\"DLC\"] >= 8).mean()\n",
    "    qas.append(make_entry(\n",
    "        f\"At least {int(HIGH_DLC_RATIO_THRESHOLD * 100)}% of frames use DLC >= 8.\",\n",
    "        high_dlc_ratio >= HIGH_DLC_RATIO_THRESHOLD,\n",
    "    ))\n",
    "    window_duration = float(df[\"Timestamp\"].iloc[-1] - df[\"Timestamp\"].iloc[0]) if total_frames > 1 else 0.0\n",
    "    qas.append(make_entry(\n",
    "        f\"The time span of this window exceeds {WINDOW_DURATION_THRESHOLD} seconds.\",\n",
    "        window_duration > WINDOW_DURATION_THRESHOLD,\n",
    "    ))\n",
    "    rare_id_ratio = (id_counts == 1).sum() / total_frames\n",
    "    qas.append(make_entry(\n",
    "        f\"More than {int(RARE_ID_RATIO_THRESHOLD * 100)}% of frames are unique IDs (possible fuzzing).\",\n",
    "        rare_id_ratio > RARE_ID_RATIO_THRESHOLD,\n",
    "    ))\n",
    "\n",
    "    dominant_id = int(id_counts.index[0])\n",
    "    dominant_group = df[df[\"ID\"] == dominant_id]\n",
    "    if not dominant_group.empty:\n",
    "        byte1_series = dominant_group[\"Byte1\"].tolist()\n",
    "        if byte1_series:\n",
    "            mode_value = int(pd.Series(byte1_series).mode().iloc[0])\n",
    "            run = 0\n",
    "            has_run = False\n",
    "            for val in byte1_series:\n",
    "                if val == mode_value:\n",
    "                    run += 1\n",
    "                    if run >= 3:\n",
    "                        has_run = True\n",
    "                        break\n",
    "                else:\n",
    "                    run = 0\n",
    "            qas.append(make_entry(\n",
    "                f\"ID {dominant_id} produces Byte1 = {mode_value} for three consecutive transmissions.\",\n",
    "                has_run,\n",
    "            ))\n",
    "        all_zero_payload = bool((dominant_group[BYTE_COLUMNS] == 0).all(axis=None))\n",
    "        qas.append(make_entry(\n",
    "            f\"ID {dominant_id} maintains an all-zero payload across the window.\",\n",
    "            all_zero_payload,\n",
    "        ))\n",
    "\n",
    "\n",
    "    duplicate_timestamps = False\n",
    "    if total_frames > 1:\n",
    "        for _, grp in df.groupby(\"Timestamp\"):\n",
    "            if len(grp[\"ID\"].unique()) > 1:\n",
    "                duplicate_timestamps = True\n",
    "                break\n",
    "    qas.append(make_entry(\n",
    "        \"Multiple different CAN IDs share the exact same timestamp in this window.\",\n",
    "        duplicate_timestamps,\n",
    "    ))\n",
    "    flood_signature = bool((max_id_share > FLOOD_SIGNATURE_SHARE) and all_small_gaps)\n",
    "    qas.append(make_entry(\n",
    "        \"The window matches the heuristic signature of a flooding attack (high share + dense timing).\",\n",
    "        flood_signature,\n",
    "    ))\n",
    "    critical_disruption = bool(critical_ids) and any(\n",
    "        (id_counts.get(cid, 0) > FLOOD_THRESHOLD) or\n",
    "        bool(((df[\"ID\"] == cid) & (df[\"Flag\"] != 0)).any())\n",
    "        for cid in critical_ids\n",
    "    )\n",
    "    qas.append(make_entry(\n",
    "        \"A critical-control ID exhibits anomalous activity in this window.\",\n",
    "        critical_disruption,\n",
    "    ))\n",
    "\n",
    "    if total_frames > 0:\n",
    "        last_flag_zero = bool(df[\"Flag\"].iloc[-1] == 0)\n",
    "        qas.append(make_entry(\n",
    "            \"Given the window where the last frame's Flag is hidden, does the final frame have Flag = 0?\",\n",
    "            last_flag_zero,\n",
    "            context_type=\"hidden_last_flag\",\n",
    "        ))\n",
    "    if total_frames > 2:\n",
    "        mid_index = total_frames // 2\n",
    "        mid_flag_zero = bool(df[\"Flag\"].iloc[mid_index] == 0)\n",
    "        qas.append(make_entry(\n",
    "            \"Given the window where the middle frame's Flag is hidden, does that frame have Flag = 0?\",\n",
    "            mid_flag_zero,\n",
    "            context_type=\"hidden_middle_flag\",\n",
    "        ))\n",
    "\n",
    "    attack_label = profile.get(\"attack_label\", \"DoS\")\n",
    "    predicted_label = rng.choice(ATTACK_LABELS)\n",
    "    qas.append(make_entry(\n",
    "        f\"This window most plausibly corresponds to the {predicted_label} attack type.\",\n",
    "        attack_label == predicted_label,\n",
    "    ))\n",
    "\n",
    "    return qas\n",
    "\n",
    "\n",
    "def iter_window_starts(num_rows: int) -> List[int]:\n",
    "    if num_rows < WINDOW_SIZE:\n",
    "        return []\n",
    "    return list(range(0, num_rows - WINDOW_SIZE + 1, WINDOW_STRIDE))\n",
    "\n",
    "\n",
    "def sample_window_indices(starts: List[int], rng: np.random.Generator) -> List[int]:\n",
    "    if not starts:\n",
    "        return []\n",
    "    sample_size = max(1, int(len(starts) * SAMPLE_RATIO))\n",
    "    sample_size = min(sample_size, len(starts))\n",
    "    return sorted(rng.choice(starts, size=sample_size, replace=False))\n",
    "\n",
    "\n",
    "def select_questions(qas: List[dict], rng: np.random.Generator) -> List[dict]:\n",
    "    if len(qas) <= QUESTIONS_PER_WINDOW:\n",
    "        return qas\n",
    "    indices = rng.choice(len(qas), size=QUESTIONS_PER_WINDOW, replace=False)\n",
    "    return [qas[i] for i in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f15e242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loaded DoS: 3665771 rows, 5 expected IDs, 3 critical IDs.\n",
      "[INFO] Loaded Fuzzy: 3838860 rows, 5 expected IDs, 3 critical IDs.\n",
      "[INFO] Loaded Gear: 4443142 rows, 5 expected IDs, 3 critical IDs.\n",
      "[INFO] Loaded RPM: 4621702 rows, 5 expected IDs, 3 critical IDs.\n",
      "[INFO] Generating questions for DoS\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc88d82554234e4d8a55af3e791b01f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "DoS windows:   0%|          | 0/1832 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] DoS: saved 5496 questions -> DoS_tf_qa/dos_questions.jsonl\n",
      "[INFO] DoS: saved JSON array -> DoS_tf_qa/dos_questions.json\n",
      "[INFO] Generating questions for Fuzzy\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4aa4e99cf5e845c6a3c93dde0f26585e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fuzzy windows:   0%|          | 0/1919 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Fuzzy: saved 5757 questions -> Fuzzy_tf_qa/fuzzy_questions.jsonl\n",
      "[INFO] Fuzzy: saved JSON array -> Fuzzy_tf_qa/fuzzy_questions.json\n",
      "[INFO] Generating questions for Gear\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2134fcdbbd741a99314d0b5c169ad96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Gear windows:   0%|          | 0/2221 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Gear: saved 6663 questions -> Gear_tf_qa/gear_questions.jsonl\n",
      "[INFO] Gear: saved JSON array -> Gear_tf_qa/gear_questions.json\n",
      "[INFO] Generating questions for RPM\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ff01c58e9af42e495fd74735780ec3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RPM windows:   0%|          | 0/2310 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] RPM: saved 6930 questions -> RPM_tf_qa/rpm_questions.jsonl\n",
      "[INFO] RPM: saved JSON array -> RPM_tf_qa/rpm_questions.json\n"
     ]
    }
   ],
   "source": [
    "# Create QA T/F dataset\n",
    "\n",
    "datasets, profiles = load_datasets({k: v for k, v in DATASET_PATHS.items() if k in SELECTED_DATASETS})\n",
    "rng = np.random.default_rng(GLOBAL_SEED)\n",
    "\n",
    "for ds_idx, (name, df) in enumerate(datasets.items()):\n",
    "    print(f\"[INFO] Generating questions for {name}\")\n",
    "    starts = iter_window_starts(len(df))\n",
    "    sampled_starts = sample_window_indices(starts, rng)\n",
    "    output_dir = Path(f\"{name}_tf_qa\")\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    q_path = output_dir / f\"{name.lower()}_questions.jsonl\"\n",
    "    q_path.write_text(\"\", encoding=\"utf-8\")  # truncate\n",
    "\n",
    "    qa_id_counter = 0\n",
    "    with q_path.open(\"a\", encoding=\"utf-8\") as f:\n",
    "        for window_idx, start in enumerate(tqdm(sampled_starts, desc=f\"{name} windows\")):\n",
    "            window = df.iloc[start:start + WINDOW_SIZE].copy().reset_index(drop=True)\n",
    "            qa_items = generate_tf_questions(window, profiles[name], rng)\n",
    "            selected_qas = select_questions(qa_items, rng)\n",
    "\n",
    "            context_full = format_window(window)\n",
    "            contexts = {\"full\": context_full}\n",
    "            if len(window) > 0:\n",
    "                contexts[\"hidden_last_flag\"] = format_window(window, hide_indices={len(window) - 1})\n",
    "            if len(window) > 2:\n",
    "                mid_index = len(window) // 2\n",
    "                contexts[\"hidden_middle_flag\"] = format_window(window, hide_indices={mid_index})\n",
    "\n",
    "            for local_q_idx, qa in enumerate(selected_qas):\n",
    "                qa_id = f\"{name}_{window_idx:06d}_{local_q_idx:02d}\"\n",
    "                context_type = qa.get(\"context_type\", \"full\")\n",
    "                context = contexts.get(context_type, context_full)\n",
    "                record = {\n",
    "                    \"qa_id\": qa_id,\n",
    "                    \"metadata\": {\n",
    "                        \"dataset\": name,\n",
    "                        \"window_index\": int(window_idx),\n",
    "                        \"window_start\": int(start),\n",
    "                        \"window_size\": int(WINDOW_SIZE),\n",
    "                    },\n",
    "                    \"context\": context,\n",
    "                    \"context_type\": context_type,\n",
    "                    \"question\": qa[\"question\"],\n",
    "                    \"ground_truth\": qa[\"answer\"],\n",
    "                }\n",
    "                f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "                qa_id_counter += 1\n",
    "\n",
    "    print(f\"[INFO] {name}: saved {qa_id_counter} questions -> {q_path}\")\n",
    "\n",
    "    json_path = q_path.with_suffix(\".json\")\n",
    "    records = []\n",
    "    with q_path.open(\"r\", encoding=\"utf-8\") as f_in:\n",
    "        for line in f_in:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                records.append(json.loads(line))\n",
    "\n",
    "    with json_path.open(\"w\", encoding=\"utf-8\") as f_out:\n",
    "        json.dump(records, f_out, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"[INFO] {name}: saved JSON array -> {json_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa491e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
