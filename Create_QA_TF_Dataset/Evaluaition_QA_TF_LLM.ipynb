{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dcc58b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5dedc479",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TARGET_MODEL_ID = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "# TARGET_MODEL_ID = \"Qwen/Qwen3-4B-Thinking-2507\"\n",
    "# TARGET_MODEL_ID = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22bfbd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION_FILES = {\n",
    "    \"DoS\":   Path(\"DoS_tf_qa/dos_questions.jsonl\"),\n",
    "    \"Fuzzy\": Path(\"Fuzzy_tf_qa/fuzzy_questions.jsonl\"),\n",
    "    \"Gear\":  Path(\"Gear_tf_qa/gear_questions.jsonl\"),\n",
    "    \"RPM\":   Path(\"RPM_tf_qa/rpm_questions.jsonl\"),\n",
    "}\n",
    "ANSWER_DIRS = {\n",
    "    \"DoS\":   Path(\"DoS_tf_qa\"),\n",
    "    \"Fuzzy\": Path(\"Fuzzy_tf_qa\"),\n",
    "    \"Gear\":  Path(\"Gear_tf_qa\"),\n",
    "    \"RPM\":   Path(\"RPM_tf_qa\"),\n",
    "}\n",
    "DATASET_NAMES = [\"DoS\", \"Fuzzy\", \"Gear\", \"RPM\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def label_to_int(label):\n",
    "    t = str(label).strip().lower()\n",
    "    if t.startswith(\"t\"):\n",
    "        return 1\n",
    "    if t.startswith(\"f\"):\n",
    "        return 0\n",
    "    return 0\n",
    "\n",
    "\n",
    "def load_questions(path: Path):\n",
    "    q_map = {}\n",
    "    if not path.exists():\n",
    "        return q_map\n",
    "\n",
    "    if path.suffix == \".json\":\n",
    "        records = json.loads(path.read_text(encoding=\"utf-8\"))\n",
    "        for rec in records:\n",
    "            q_map[rec[\"qa_id\"]] = {\n",
    "                \"question\": rec[\"question\"],\n",
    "                \"ground_truth\": rec[\"ground_truth\"],\n",
    "            }\n",
    "    else:  # jsonl\n",
    "        with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                rec = json.loads(line)\n",
    "                q_map[rec[\"qa_id\"]] = {\n",
    "                    \"question\": rec[\"question\"],\n",
    "                    \"ground_truth\": rec[\"ground_truth\"],\n",
    "                }\n",
    "    return q_map\n",
    "\n",
    "\n",
    "def load_answers_jsonl(path: Path):\n",
    "    records = []\n",
    "    if not path.exists():\n",
    "        return records\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                records.append(json.loads(line))\n",
    "    return records\n",
    "\n",
    "\n",
    "def compute_stats(records):\n",
    "    stats = defaultdict(lambda: {\"total\": 0, \"correct\": 0, \"pred\": [], \"gold\": []})\n",
    "    for rec in records:\n",
    "        q = rec[\"question\"]\n",
    "        pred = rec.get(\"llm_answer\")\n",
    "        gold = rec.get(\"ground_truth\")\n",
    "        stats[q][\"total\"] += 1\n",
    "        stats[q][\"correct\"] += int(pred == gold)\n",
    "        stats[q][\"pred\"].append(pred)\n",
    "        stats[q][\"gold\"].append(gold)\n",
    "    return stats\n",
    "\n",
    "\n",
    "def f1_for_question(info):\n",
    "    if not info[\"gold\"]:\n",
    "        return 0.0\n",
    "    y_true = [label_to_int(v) for v in info[\"gold\"]]\n",
    "    y_pred = [label_to_int(v) for v in info[\"pred\"]]\n",
    "    return f1_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "213a30a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saved performance for model 'Qwen/Qwen3-4B-Thinking-2507' to LLM_TF_Performce/Performance_TF_Qwen_Qwen3_4B_Thinking_2507.csv\n"
     ]
    }
   ],
   "source": [
    "# model_data[dataset] = list[record]\n",
    "model_data = defaultdict(list)\n",
    "\n",
    "for ds_name in DATASET_NAMES:\n",
    "    q_path = QUESTION_FILES[ds_name]\n",
    "    q_map = load_questions(q_path)\n",
    "    if not q_map:\n",
    "        print(f\"[WARN] No questions for {ds_name} at {q_path}\")\n",
    "        continue\n",
    "\n",
    "    ans_dir = ANSWER_DIRS[ds_name]\n",
    "    if not ans_dir.exists():\n",
    "        print(f\"[WARN] Answer dir for {ds_name} not found at {ans_dir}\")\n",
    "        continue\n",
    "\n",
    "    for ans_path in ans_dir.glob(\"*answers*.jsonl\"):\n",
    "        ans_records = load_answers_jsonl(ans_path)\n",
    "        if not ans_records:\n",
    "            continue\n",
    "        for rec in ans_records:\n",
    "            if rec.get(\"model\") != TARGET_MODEL_ID:\n",
    "                continue\n",
    "            qa_id = rec[\"qa_id\"]\n",
    "            if qa_id not in q_map:\n",
    "                continue\n",
    "            q_info = q_map[qa_id]\n",
    "            model_data[ds_name].append({\n",
    "                \"dataset\": ds_name,\n",
    "                \"question\": q_info[\"question\"],\n",
    "                \"ground_truth\": q_info[\"ground_truth\"],\n",
    "                \"llm_answer\": rec.get(\"llm_answer\"),\n",
    "            })\n",
    "\n",
    "if not model_data:\n",
    "    print(f\"[WARN] No records found for model: {TARGET_MODEL_ID}\")\n",
    "else:\n",
    "    model_tag = (\n",
    "    TARGET_MODEL_ID.replace(\"/\", \"_\")\n",
    "                   .replace(\":\", \"_\")\n",
    "                   .replace(\"-\", \"_\")\n",
    "                   .replace(\" \", \"_\")\n",
    "    )\n",
    "\n",
    "    output_dir = Path(\"LLM_TF_Performce\")\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    out_path = output_dir / f\"Performance_TF_{model_tag}.csv\"\n",
    "\n",
    "\n",
    "    with out_path.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"Attack\", \"Question\", \"Total\", \"Correct\", \"Accuracy\", \"F1\"])\n",
    "\n",
    "        combined_records = []\n",
    "\n",
    "        for ds_name in sorted(model_data.keys()):\n",
    "            records = model_data[ds_name]\n",
    "            combined_records.extend(records)\n",
    "            stats = compute_stats(records)\n",
    "\n",
    "            for question, info in stats.items():\n",
    "                total = info[\"total\"]\n",
    "                correct = info[\"correct\"]\n",
    "                acc = correct / total if total else 0.0\n",
    "                f1 = f1_for_question(info)\n",
    "                writer.writerow(\n",
    "                    [ds_name, question, total, correct, f\"{acc:.3%}\", f\"{f1:.3f}\"]\n",
    "                )\n",
    "\n",
    "            ds_total = sum(info[\"total\"] for info in stats.values())\n",
    "            ds_correct = sum(info[\"correct\"] for info in stats.values())\n",
    "            y_true_all, y_pred_all = [], []\n",
    "            for info in stats.values():\n",
    "                y_true_all.extend(label_to_int(v) for v in info[\"gold\"])\n",
    "                y_pred_all.extend(label_to_int(v) for v in info[\"pred\"])\n",
    "            if ds_total > 0 and y_true_all:\n",
    "                acc_total = ds_correct / ds_total\n",
    "                f1_total = f1_score(y_true_all, y_pred_all, average=\"macro\", zero_division=0)\n",
    "            else:\n",
    "                acc_total = 0.0\n",
    "                f1_total = 0.0\n",
    "            writer.writerow(\n",
    "                [f\"{ds_name}_total\", \"TOTAL\", ds_total, ds_correct,\n",
    "                 f\"{acc_total:.3%}\", f\"{f1_total:.3f}\"]\n",
    "            )\n",
    "\n",
    "        if combined_records:\n",
    "            stats_c = compute_stats(combined_records)\n",
    "            for question, info in stats_c.items():\n",
    "                total = info[\"total\"]\n",
    "                correct = info[\"correct\"]\n",
    "                acc = correct / total if total else 0.0\n",
    "                f1 = f1_for_question(info)\n",
    "                writer.writerow(\n",
    "                    [\"Combined\", question, total, correct, f\"{acc:.3%}\", f\"{f1:.3f}\"]\n",
    "                )\n",
    "\n",
    "            cb_total = sum(info[\"total\"] for info in stats_c.values())\n",
    "            cb_correct = sum(info[\"correct\"] for info in stats_c.values())\n",
    "            y_true_all, y_pred_all = [], []\n",
    "            for info in stats_c.values():\n",
    "                y_true_all.extend(label_to_int(v) for v in info[\"gold\"])\n",
    "                y_pred_all.extend(label_to_int(v) for v in info[\"pred\"])\n",
    "            if cb_total > 0 and y_true_all:\n",
    "                acc_total = cb_correct / cb_total\n",
    "                f1_total = f1_score(y_true_all, y_pred_all, average=\"macro\", zero_division=0)\n",
    "            else:\n",
    "                acc_total = 0.0\n",
    "                f1_total = 0.0\n",
    "            writer.writerow(\n",
    "                [\"Combined_total\", \"TOTAL\", cb_total, cb_correct,\n",
    "                 f\"{acc_total:.3%}\", f\"{f1_total:.3f}\"]\n",
    "            )\n",
    "\n",
    "    print(f\"[INFO] Saved performance for model '{TARGET_MODEL_ID}' to {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1aac46a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
