{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89479e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6530f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION_FILES = {\n",
    "    \"DoS\":   Path(\"DoS_tf_qa/dos_questions.json\"),\n",
    "    \"Fuzzy\": Path(\"Fuzzy_tf_qa/fuzzy_questions.json\"),\n",
    "    \"Gear\":  Path(\"Gear_tf_qa/gear_questions.json\"),\n",
    "    \"RPM\":   Path(\"RPM_tf_qa/rpm_questions.json\"),\n",
    "}\n",
    "SELECTED_DATASETS = [\"DoS\", \"Fuzzy\", \"Gear\", \"RPM\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a350ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "DTYPE = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    "MODEL_TAG = MODEL_ID.split(\"/\")[-1].replace(\".\", \"_\").replace(\"-\", \"_\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=DTYPE,\n",
    "    device_map=\"auto\",\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "085ec0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_ID = \"Qwen/Qwen3-4B-Thinking-2507\"  # OR \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "# DTYPE = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    "# MODEL_TAG = MODEL_ID.split(\"/\")[-1].replace(\".\", \"_\").replace(\"-\", \"_\")\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True, trust_remote_code=True)\n",
    "# if tokenizer.pad_token is None:\n",
    "#     tokenizer.pad_token = tokenizer.eos_token\n",
    "# tokenizer.padding_side = \"left\"\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     MODEL_ID,\n",
    "#     torch_dtype=DTYPE,\n",
    "#     device_map=\"auto\",\n",
    "#     pad_token_id=tokenizer.pad_token_id,\n",
    "#     trust_remote_code=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c317c4b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac9caf662e5b4f6fa864a23e1a2dbee9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# MODEL_ID = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "# DTYPE = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    "# MODEL_TAG = MODEL_ID.split(\"/\")[-1].replace(\".\", \"_\").replace(\"-\", \"_\")\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\n",
    "#     MODEL_ID,\n",
    "#     use_fast=True,\n",
    "#     trust_remote_code=True,\n",
    "# )\n",
    "# if tokenizer.pad_token is None:\n",
    "#     tokenizer.pad_token = tokenizer.eos_token\n",
    "# tokenizer.padding_side = \"left\"\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     MODEL_ID,\n",
    "#     torch_dtype=DTYPE,\n",
    "#     device_map=\"auto\",\n",
    "#     pad_token_id=tokenizer.pad_token_id,\n",
    "#     trust_remote_code=True,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6a0e5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt_text(context: str, question: str, context_type: str = \"full\") -> str:\n",
    "    extra_instruction = \"\"\n",
    "    if context_type == \"hidden_last_flag\":\n",
    "        extra_instruction = \"Use the preceding frames to infer the hidden flag of the final frame.\\n\"\n",
    "    elif context_type == \"hidden_middle_flag\":\n",
    "        extra_instruction = \"Use the frames before and after the hidden middle frame to infer its flag.\\n\"\n",
    "\n",
    "    system_prompt = (\n",
    "        \"You are a CAN bus intrusion-detection analyst. Study timestamp ordering, ID frequency, \"\n",
    "        \"payload stability, byte ranges, and gaps between frames. Use those characteristics plus the \"\n",
    "        \"statement to reach a True/False conclusion. Respond with True or False only.\"\n",
    "    )\n",
    "    user_prompt = (\n",
    "        \"Below is a CAN bus time window. Review the sequence carefully, note anomalies or missing IDs, \"\n",
    "        \"and reason about the claim.\\n\"\n",
    "        f\"{context}\\n\\n{extra_instruction}\"\n",
    "        f\"Statement: {question}\\nAnswer True or False only.\"\n",
    "    )\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "\n",
    "\n",
    "def normalize_tf_answer(text: str) -> str:\n",
    "    if not text:\n",
    "        return \"False\"\n",
    "    first = text.strip().split()[0].replace(\".\", \"\").lower()\n",
    "    if first.startswith(\"t\"):\n",
    "        return \"True\"\n",
    "    if first.startswith(\"f\"):\n",
    "        return \"False\"\n",
    "    return \"False\"\n",
    "\n",
    "\n",
    "def query_llm(context: str, question: str, context_type: str = \"full\",\n",
    "              max_new_tokens: int = 16) -> str:\n",
    "    prompt_text = build_prompt_text(context, question, context_type=context_type)\n",
    "    inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            temperature=None,\n",
    "            top_p=None,\n",
    "            top_k=None,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    completion = tokenizer.decode(\n",
    "        outputs[0][inputs[\"input_ids\"].shape[-1]:],\n",
    "        skip_special_tokens=True,\n",
    "    ).strip()\n",
    "    return normalize_tf_answer(completion)\n",
    "\n",
    "\n",
    "def load_questions(path: Path) -> List[dict]:\n",
    "    if not path.exists():\n",
    "        return []\n",
    "    if path.suffix == \".json\":\n",
    "        with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            return json.load(f)  \n",
    "\n",
    "    records = []\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                records.append(json.loads(line))\n",
    "    return records\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08ff1c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] DoS: loaded 5496 questions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DoS answering: 100%|██████████| 5496/5496 [1:59:37<00:00,  1.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] DoS: answers saved to DoS_tf_qa/dos_answers_DeepSeek_R1_Distill_Llama_8B.jsonl\n",
      "[INFO] Fuzzy: loaded 5757 questions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fuzzy answering: 100%|██████████| 5757/5757 [2:05:01<00:00,  1.30s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Fuzzy: answers saved to Fuzzy_tf_qa/fuzzy_answers_DeepSeek_R1_Distill_Llama_8B.jsonl\n",
      "[INFO] Gear: loaded 6663 questions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gear answering: 100%|██████████| 6663/6663 [2:24:58<00:00,  1.31s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Gear: answers saved to Gear_tf_qa/gear_answers_DeepSeek_R1_Distill_Llama_8B.jsonl\n",
      "[INFO] RPM: loaded 6930 questions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RPM answering: 100%|██████████| 6930/6930 [2:30:37<00:00,  1.30s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] RPM: answers saved to RPM_tf_qa/rpm_answers_DeepSeek_R1_Distill_Llama_8B.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for ds_name in SELECTED_DATASETS:\n",
    "    q_path = Path(QUESTION_FILES[ds_name])\n",
    "    if not q_path.exists():\n",
    "        print(f\"[WARN] Questions for {ds_name} not found at {q_path}, skip.\")\n",
    "        continue\n",
    "\n",
    "    questions = load_questions(q_path)\n",
    "    print(f\"[INFO] {ds_name}: loaded {len(questions)} questions.\")\n",
    "\n",
    "    out_dir = q_path.parent\n",
    "    ans_path = out_dir / f\"{ds_name.lower()}_answers_{MODEL_TAG}.jsonl\"\n",
    "\n",
    "    ans_path.write_text(\"\", encoding=\"utf-8\")  # truncate\n",
    "    with ans_path.open(\"a\", encoding=\"utf-8\") as f:\n",
    "        for rec in tqdm(questions, desc=f\"{ds_name} answering\"):\n",
    "            qa_id = rec[\"qa_id\"]\n",
    "            dataset = rec[\"metadata\"][\"dataset\"]\n",
    "            ctx_type = rec.get(\"context_type\", \"full\")\n",
    "            ctx = rec[\"context\"]\n",
    "            q_text = rec[\"question\"]\n",
    "            gt = rec[\"ground_truth\"]\n",
    "\n",
    "            pred = query_llm(ctx, q_text, context_type=ctx_type)\n",
    "            answer_rec = {\n",
    "                \"qa_id\": qa_id,\n",
    "                \"dataset\": dataset,\n",
    "                \"model\": MODEL_ID,\n",
    "                \"llm_answer\": pred,\n",
    "                \"ground_truth\": gt,\n",
    "                \"is_correct\": pred == gt,\n",
    "                \"answer_valid\": bool(pred),\n",
    "            }\n",
    "            f.write(json.dumps(answer_rec, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"[INFO] {ds_name}: answers saved to {ans_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2192709c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
