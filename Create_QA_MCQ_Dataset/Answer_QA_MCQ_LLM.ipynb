{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61930757",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750a14c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "MCQ_QUESTION_FILES = {\n",
    "    \"DoS\":   Path(\"DoS_mcq_qa/dos_mcq_questions.json\"),\n",
    "    \"Fuzzy\": Path(\"Fuzzy_mcq_qa/fuzzy_mcq_questions.json\"),\n",
    "    \"Gear\":  Path(\"Gear_mcq_qa/gear_mcq_questions.json\"),\n",
    "    \"RPM\":   Path(\"RPM_mcq_qa/rpm_mcq_questions.json\"),\n",
    "}\n",
    "\n",
    "SELECTED_DATASETS = [\"DoS\", \"Fuzzy\", \"Gear\", \"RPM\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1023a046",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "DTYPE = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    "MODEL_TAG = MODEL_ID.split(\"/\")[-1].replace(\".\", \"_\").replace(\"-\", \"_\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=DTYPE,\n",
    "    device_map=\"auto\",\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95eabcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example: Qwen 3-4B Thinking\n",
    "# MODEL_ID = \"Qwen/Qwen3-4B-Thinking-2507\"  # or \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "# DTYPE = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    "# MODEL_TAG = MODEL_ID.split(\"/\")[-1].replace(\".\", \"_\").replace(\"-\", \"_\")\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True, trust_remote_code=True)\n",
    "# if tokenizer.pad_token is None:\n",
    "#     tokenizer.pad_token = tokenizer.eos_token\n",
    "# tokenizer.padding_side = \"left\"\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     MODEL_ID,\n",
    "#     torch_dtype=DTYPE,\n",
    "#     device_map=\"auto\",\n",
    "#     pad_token_id=tokenizer.pad_token_id,\n",
    "#     trust_remote_code=True,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ae054ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19c27cf390da44f1accfc8baeac6e447",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# MODEL_ID = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "# DTYPE = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    "# MODEL_TAG = MODEL_ID.split(\"/\")[-1].replace(\".\", \"_\").replace(\"-\", \"_\")\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\n",
    "#     MODEL_ID,\n",
    "#     use_fast=True,\n",
    "#     trust_remote_code=True,\n",
    "# )\n",
    "# if tokenizer.pad_token is None:\n",
    "#     tokenizer.pad_token = tokenizer.eos_token\n",
    "# tokenizer.padding_side = \"left\"\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     MODEL_ID,\n",
    "#     torch_dtype=DTYPE,\n",
    "#     device_map=\"auto\",\n",
    "#     pad_token_id=tokenizer.pad_token_id,\n",
    "#     trust_remote_code=True,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "085aae8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mcq_prompt_text(context: str,\n",
    "                          question: str,\n",
    "                          options: Dict[str, str]) -> str:\n",
    "    \"\"\"\n",
    "    Build a chat-style prompt for MCQ answering.\n",
    "    Model is expected to reply with a single letter: A/B/C/D.\n",
    "    \"\"\"\n",
    "    # Ensure options are ordered A-D\n",
    "    ordered_letters = [\"A\", \"B\", \"C\", \"D\"]\n",
    "    lines = []\n",
    "    for letter in ordered_letters:\n",
    "        if letter in options:\n",
    "            lines.append(f\"{letter}. {options[letter]}\")\n",
    "    options_block = \"\\n\".join(lines)\n",
    "\n",
    "    system_prompt = (\n",
    "        \"You are a CAN bus intrusion-detection analyst. \"\n",
    "        \"Given a CAN bus time window and a multiple-choice question about attack behavior, \"\n",
    "        \"ID frequency, timing, payload, or flag patterns, carefully reason and choose the single best answer. \"\n",
    "        \"Respond with only the option letter: A, B, C, or D.\"\n",
    "    )\n",
    "\n",
    "    user_prompt = (\n",
    "        \"Below is a CAN bus time window. Review the sequence carefully, note anomalies or missing IDs, \"\n",
    "        \"and then answer the multiple-choice question.\\n\\n\"\n",
    "        f\"{context}\\n\\n\"\n",
    "        f\"Question: {question}\\n\\n\"\n",
    "        f\"Options:\\n{options_block}\\n\\n\"\n",
    "        \"Answer with only a single letter: A, B, C, or D.\"\n",
    "    )\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=False,\n",
    "    )\n",
    "\n",
    "\n",
    "def normalize_mcq_answer(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize raw model text to one of {'A','B','C','D'}.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"A\"\n",
    "    s = text.strip()\n",
    "    # First non-whitespace character\n",
    "    first = s[0].upper()\n",
    "    if first in {\"A\", \"B\", \"C\", \"D\"}:\n",
    "        return first\n",
    "    # Fallback: scan the string for A-D\n",
    "    for ch in s.upper():\n",
    "        if ch in {\"A\", \"B\", \"C\", \"D\"}:\n",
    "            return ch\n",
    "    # Default fallback\n",
    "    return \"A\"\n",
    "\n",
    "\n",
    "def query_mcq_llm(context: str,\n",
    "                  question: str,\n",
    "                  options: Dict[str, str],\n",
    "                  max_new_tokens: int = 8) -> str:\n",
    "    prompt_text = build_mcq_prompt_text(context, question, options)\n",
    "    inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            temperature=None,\n",
    "            top_p=None,\n",
    "            top_k=None,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    completion = tokenizer.decode(\n",
    "        outputs[0][inputs[\"input_ids\"].shape[-1]:],\n",
    "        skip_special_tokens=True,\n",
    "    ).strip()\n",
    "    return normalize_mcq_answer(completion)\n",
    "\n",
    "\n",
    "def load_questions(path: Path) -> List[dict]:\n",
    "    \"\"\"\n",
    "    Load MCQ questions from .json (list) or .jsonl.\n",
    "    \"\"\"\n",
    "    if not path.exists():\n",
    "        return []\n",
    "    if path.suffix == \".json\":\n",
    "        with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            return json.load(f)\n",
    "    # jsonl fallback\n",
    "    records = []\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            records.append(json.loads(line))\n",
    "    return records\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22013af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] DoS: loaded 5496 MCQ questions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DoS MCQ answering: 100%|██████████| 5496/5496 [1:36:07<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] DoS: MCQ answers saved to DoS_mcq_qa/dos_mcq_answers_DeepSeek_R1_Distill_Llama_8B.jsonl\n",
      "[INFO] Fuzzy: loaded 5757 MCQ questions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fuzzy MCQ answering: 100%|██████████| 5757/5757 [1:40:45<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Fuzzy: MCQ answers saved to Fuzzy_mcq_qa/fuzzy_mcq_answers_DeepSeek_R1_Distill_Llama_8B.jsonl\n",
      "[INFO] Gear: loaded 6663 MCQ questions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gear MCQ answering: 100%|██████████| 6663/6663 [1:56:42<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Gear: MCQ answers saved to Gear_mcq_qa/gear_mcq_answers_DeepSeek_R1_Distill_Llama_8B.jsonl\n",
      "[INFO] RPM: loaded 6930 MCQ questions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RPM MCQ answering: 100%|██████████| 6930/6930 [2:27:44<00:00,  1.28s/it]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] RPM: MCQ answers saved to RPM_mcq_qa/rpm_mcq_answers_DeepSeek_R1_Distill_Llama_8B.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for ds_name in SELECTED_DATASETS:\n",
    "    q_path = MCQ_QUESTION_FILES[ds_name]\n",
    "    if not q_path.exists():\n",
    "        print(f\"[WARN] MCQ questions for {ds_name} not found at {q_path}, skip.\")\n",
    "        continue\n",
    "\n",
    "    questions = load_questions(q_path)\n",
    "    print(f\"[INFO] {ds_name}: loaded {len(questions)} MCQ questions.\")\n",
    "\n",
    "    out_dir = q_path.parent\n",
    "    ans_path = out_dir / f\"{ds_name.lower()}_mcq_answers_{MODEL_TAG}.jsonl\"\n",
    "\n",
    "    # Truncate old answer file\n",
    "    ans_path.write_text(\"\", encoding=\"utf-8\")\n",
    "\n",
    "    with ans_path.open(\"a\", encoding=\"utf-8\") as f:\n",
    "        for rec in tqdm(questions, desc=f\"{ds_name} MCQ answering\"):\n",
    "            qa_id = rec.get(\"qa_id\")\n",
    "            metadata = rec.get(\"metadata\", {})\n",
    "            dataset = metadata.get(\"dataset\", ds_name)\n",
    "            context = rec[\"context\"]\n",
    "            question = rec[\"question\"]\n",
    "            options = rec[\"options\"]\n",
    "            gt = rec[\"answer\"]  # correct option letter, e.g. \"B\"\n",
    "\n",
    "            pred = query_mcq_llm(context, question, options)\n",
    "            answer_rec = {\n",
    "                \"qa_id\": qa_id,\n",
    "                \"dataset\": dataset,\n",
    "                \"mcq_type\": rec.get(\"mcq_type\"),\n",
    "                \"model\": MODEL_ID,\n",
    "                \"llm_answer\": pred,\n",
    "                \"ground_truth\": gt,\n",
    "                \"is_correct\": pred == gt,\n",
    "                \"answer_valid\": pred in {\"A\", \"B\", \"C\", \"D\"},\n",
    "            }\n",
    "            f.write(json.dumps(answer_rec, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"[INFO] {ds_name}: MCQ answers saved to {ans_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
