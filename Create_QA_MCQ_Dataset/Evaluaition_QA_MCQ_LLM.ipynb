{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5515655d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7a0a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MCQ_DATASET_NAMES = [\"DoS\", \"Fuzzy\", \"Gear\", \"RPM\"]\n",
    "\n",
    "MCQ_QUESTION_FILES = {\n",
    "    \"DoS\":   Path(\"DoS_mcq_qa/dos_mcq_questions.json\"),\n",
    "    \"Fuzzy\": Path(\"Fuzzy_mcq_qa/fuzzy_mcq_questions.json\"),\n",
    "    \"Gear\":  Path(\"Gear_mcq_qa/gear_mcq_questions.json\"),\n",
    "    \"RPM\":   Path(\"RPM_mcq_qa/rpm_mcq_questions.json\"),\n",
    "}\n",
    "\n",
    "\n",
    "MODEL_ID = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "# MODEL_ID = \"Qwen/Qwen3-4B-Thinking-2507\" \n",
    "# MODEL_ID = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "\n",
    "MODEL_TAG = MODEL_ID.split(\"/\")[-1].replace(\".\", \"_\").replace(\"-\", \"_\")\n",
    "\n",
    "def mcq_answer_path(dataset_name: str, model_tag: str) -> Path:\n",
    "    return Path(f\"{dataset_name}_mcq_qa/{dataset_name.lower()}_mcq_answers_{model_tag}.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6b1966",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mcq_questions_map(path: Path):\n",
    "    \"\"\"\n",
    "    return {qa_id: question_text}\n",
    "    \"\"\"\n",
    "    if not path.exists():\n",
    "        return {}\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)  # list[dict]\n",
    "    qmap = {}\n",
    "    for rec in data:\n",
    "        qa_id = rec.get(\"qa_id\")\n",
    "        question = rec.get(\"question\", \"\")\n",
    "        if qa_id is not None:\n",
    "            qmap[qa_id] = question\n",
    "    return qmap\n",
    "\n",
    "\n",
    "def load_mcq_answers(path: Path):\n",
    "    records = []\n",
    "    if not path.exists():\n",
    "        return records\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            records.append(json.loads(line))\n",
    "    return records\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3e8b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stats(records):\n",
    "    \"\"\"\n",
    "    records: list of dict with keys: question, pred, gold\n",
    "    per-question :\n",
    "      stats[question] = {\n",
    "        \"total\": int,\n",
    "        \"correct\": int,\n",
    "        \"pred\": list[str],\n",
    "        \"gold\": list[str],\n",
    "        \"labels\": set[str],\n",
    "      }\n",
    "    \"\"\"\n",
    "    stats = defaultdict(lambda: {\"total\": 0, \"correct\": 0,\n",
    "                                \"pred\": [], \"gold\": [], \"labels\": set()})\n",
    "    for rec in records:\n",
    "        q = rec[\"question\"]\n",
    "        pred = rec[\"pred\"]\n",
    "        gold = rec[\"gold\"]\n",
    "        stats[q][\"total\"] += 1\n",
    "        stats[q][\"correct\"] += int(pred == gold)\n",
    "        stats[q][\"pred\"].append(pred)\n",
    "        stats[q][\"gold\"].append(gold)\n",
    "        stats[q][\"labels\"].update([pred, gold])\n",
    "    return stats\n",
    "\n",
    "\n",
    "def f1_for_question(info):\n",
    "    labels = list(info[\"labels\"])\n",
    "    if not labels:\n",
    "        return 0.0\n",
    "    if len(labels) == 1:\n",
    "        return 1.0 if info[\"correct\"] == info[\"total\"] else 0.0\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(labels)\n",
    "    y_true = encoder.transform(info[\"gold\"])\n",
    "    y_pred = encoder.transform(info[\"pred\"])\n",
    "    return f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ab1531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] MCQ performance saved to LLM_MCQ_Performce/Performance_MCQ_DeepSeek_R1_Distill_Llama_8B.csv\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_DIR = Path(\"LLM_MCQ_Performce\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "out_csv = OUTPUT_DIR / f\"Performance_MCQ_{MODEL_TAG}.csv\"\n",
    "\n",
    "header = [\"Attack\", \"Question\", \"Total\", \"Correct\", \"Accuracy\", \"F1\"]\n",
    "\n",
    "all_dataset_records = []\n",
    "\n",
    "with out_csv.open(\"w\", encoding=\"utf-8\", newline=\"\") as f_out:\n",
    "    writer = csv.writer(f_out)\n",
    "    writer.writerow(header)\n",
    "\n",
    "    for name in MCQ_DATASET_NAMES:\n",
    "        q_path = MCQ_QUESTION_FILES[name]\n",
    "        a_path = mcq_answer_path(name, MODEL_TAG)\n",
    "\n",
    "        qmap = load_mcq_questions_map(q_path)\n",
    "        ans_records = load_mcq_answers(a_path)\n",
    "\n",
    "        if not qmap or not ans_records:\n",
    "            print(f\"[WARN] Skip {name}: questions or answers missing.\")\n",
    "            continue\n",
    "\n",
    "        ds_records = []\n",
    "        for rec in ans_records:\n",
    "            qa_id = rec[\"qa_id\"]\n",
    "            q_text = qmap.get(qa_id, \"\")\n",
    "            pred = str(rec.get(\"llm_answer\", \"\")).strip().upper()\n",
    "            gold = str(rec.get(\"ground_truth\", \"\")).strip().upper()\n",
    "            if pred not in {\"A\", \"B\", \"C\", \"D\"}:\n",
    "                continue\n",
    "            if gold not in {\"A\", \"B\", \"C\", \"D\"}:\n",
    "                continue\n",
    "            ds_records.append({\"question\": q_text, \"pred\": pred, \"gold\": gold})\n",
    "\n",
    "        if not ds_records:\n",
    "            print(f\"[WARN] No valid MCQ records for {name}.\")\n",
    "            continue\n",
    "\n",
    "        all_dataset_records.extend(ds_records)\n",
    "\n",
    "        stats = compute_stats(ds_records)\n",
    "        for question, info in stats.items():\n",
    "            total = info[\"total\"]       \n",
    "            correct = info[\"correct\"]  \n",
    "            acc = correct / total if total else 0.0\n",
    "            f1q = f1_for_question(info)\n",
    "            writer.writerow([\n",
    "                name,\n",
    "                question,\n",
    "                total,\n",
    "                correct,\n",
    "                f\"{acc:.3f}\",\n",
    "                f\"{f1q:.3f}\",\n",
    "            ])\n",
    "\n",
    "        ds_total_pred = [r[\"pred\"] for r in ds_records]\n",
    "        ds_total_gold = [r[\"gold\"] for r in ds_records]\n",
    "        total = len(ds_records)                        \n",
    "        correct = sum(int(p == g) for p, g in zip(ds_total_pred, ds_total_gold))\n",
    "        acc = correct / total if total else 0.0\n",
    "        if ds_total_pred:\n",
    "            encoder = LabelEncoder()\n",
    "            encoder.fit(ds_total_gold + ds_total_pred)\n",
    "            y_true = encoder.transform(ds_total_gold)\n",
    "            y_pred = encoder.transform(ds_total_pred)\n",
    "            f1_ds = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "        else:\n",
    "            f1_ds = 0.0\n",
    "\n",
    "        writer.writerow([\n",
    "            f\"{name}_total\",\n",
    "            \"TOTAL\",\n",
    "            total,\n",
    "            correct,\n",
    "            f\"{acc:.3f}\",\n",
    "            f\"{f1_ds:.3f}\",\n",
    "        ])\n",
    "\n",
    "    if all_dataset_records:\n",
    "        combined_stats = compute_stats(all_dataset_records)\n",
    "        for question, info in combined_stats.items():\n",
    "            total = info[\"total\"]        \n",
    "            correct = info[\"correct\"]    \n",
    "            acc = correct / total if total else 0.0\n",
    "            f1q = f1_for_question(info)\n",
    "            writer.writerow([\n",
    "                \"Combined\",          \n",
    "                question,\n",
    "                total,\n",
    "                correct,\n",
    "                f\"{acc:.3f}\",\n",
    "                f\"{f1q:.3f}\",\n",
    "            ])\n",
    "\n",
    "        comb_pred = [r[\"pred\"] for r in all_dataset_records]\n",
    "        comb_gold = [r[\"gold\"] for r in all_dataset_records]\n",
    "        total = len(all_dataset_records)\n",
    "        correct = sum(int(p == g) for p, g in zip(comb_pred, comb_gold))\n",
    "        acc = correct / total if total else 0.0\n",
    "\n",
    "        encoder = LabelEncoder()\n",
    "        encoder.fit(comb_gold + comb_pred)\n",
    "        y_true = encoder.transform(comb_gold)\n",
    "        y_pred = encoder.transform(comb_pred)\n",
    "        f1_comb = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "\n",
    "        writer.writerow([\n",
    "            \"Combined_total\",\n",
    "            \"TOTAL\",\n",
    "            total,\n",
    "            correct,\n",
    "            f\"{acc:.3f}\",\n",
    "            f\"{f1_comb:.3f}\",\n",
    "        ])\n",
    "\n",
    "\n",
    "print(f\"[INFO] MCQ performance saved to {out_csv}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
